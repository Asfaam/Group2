# -*- coding: utf-8 -*-
"""FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xBx-buHFJ5Xq9iNjqz08eiJ8ZZKdODEn
"""

import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score
from sklearn.metrics import confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from keras.models import Model
from keras.layers import Input, Dense

from google.colab import drive
drive.mount('/content/drive')
# Load the dataset
breast_Cancer = pd.read_csv( '/content/drive/My Drive/Colab Notebooks/AI Final Project/breast-cancer.csv')

breast_Cancer.info()  #Viewing the dataset info to know their various datatype

"""## Dropping the "id"because it doesn't have any impact on the dataset"""

breast_Cancer = breast_Cancer.drop('id', axis=1)

breast_Cancer #Display the entire the dataset

"""# Checking if there are null and missing values in the dataset"""

breast_Cancer.isnull().sum()

"""## #Rename Dataset to Label to make it easy to understand"""

breast_Cancer = breast_Cancer.rename(columns={'Diagnosis':'Label'})
breast_Cancer.dtypes

breast_Cancer.nunique() #Checking the uniqueness of each variables in the dataset

breast_Cancer.head()

"""#        Encoding the Diagnosis where M=1 and B=0"""

# Create a label encoder
label_encoder = LabelEncoder()

# Encode the 'diagnosis' column
breast_Cancer['diagnosis'] = label_encoder.fit_transform(breast_Cancer['diagnosis'])

# Display the DataFrame with the converted 'diagnosis' column
breast_Cancer

breast_Cancer.describe()

corr = breast_Cancer.corr()
corr["diagnosis"]

"""# Virtualizing the correlations between the features variables and Diagnosis"""

# Create a heatmap to visualize correlation
plt.figure(figsize=(16, 16))
sns.heatmap(breast_Cancer.corr(), cmap='coolwarm', annot=True, fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

"""From the above heatmap, it is observed that the radius_mean, perimeter_mean, area_mean considering only one among these, compactness_mean, concavity_mean, concave_points_mean are highly correlated whereas texture_mean and smoothness_mean, symmetry_mean and fractal_dimension_mean are less correlated compared to the features mentioned before.

Also, the other features like radius_se, perimeter_se, area_se are highly correlated considering only one among these whereas compactness_se, concavity_se and concave_points_se are little less correlated compared to the features mentioned before.

However, we remove the features that are highly correlated because high correlated features effect and do not improve the models like linear regression, random forests.

Removing such type of correlated features improves the learning algorithm faster. Considering less features mean high improvement in speed.
"""

import pandas as pd

# Assuming 'breast_Cancer' is your DataFrame
correlation_matrix = breast_Cancer.corr()

# Set a threshold for correlation
threshold = 0.5

# Select features with correlation above the threshold
high_correlation_features = (correlation_matrix['diagnosis'].abs() >= threshold)

# Display selected features
selected_features = correlation_matrix.index[high_correlation_features].tolist()
selected_features
processed_data=breast_Cancer[selected_features]

breast_Cancer.info()

processed_data.info()

X = processed_data.drop('diagnosis', axis=1)
y = processed_data['diagnosis']

scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)

# Split the data into training and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

X

from keras.layers import Input, Dense
from keras.models import Model

# Define the function to create the DNN model
def create_dnn_model():
    # Input layer
    input_layer = Input(shape=(X_train.shape[1],))

    # Hidden layers
    hidden1 = Dense(128, activation='relu')(input_layer)
    hidden2 = Dense(64, activation='relu')(hidden1)
    hidden3 = Dense(32, activation='relu')(hidden2)
    hidden4 = Dense(16, activation='relu')(hidden3)
    hidden5 = Dense(8, activation='relu')(hidden4)

    # Output layer
    output_layer = Dense(1, activation='sigmoid')(hidden5)

    # Create the model
    model = Model(inputs=input_layer, outputs=output_layer)

    # Compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Create the model
dnn_model = create_dnn_model()

# Display the model summary
dnn_model.summary()

# Train the model
dnn_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)

# Evaluate the model
y_pred = dnn_model.predict(X_test)
y_pred_binary = (y_pred > 0.5).astype(int)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred_binary)
roc_auc = roc_auc_score(y_test, y_pred)
recall = recall_score(y_test, y_pred_binary)
precision = precision_score(y_test, y_pred_binary)

# Display metrics
print(f'Accuracy: {accuracy:.4f}')
print(f'AUC Score: {roc_auc:.4f}')
print(f'Recall: {recall:.4f}')
print(f'Precision: {precision:.4f}')

# Evaluate the model on the train set
test_loss, test_accuracy = dnn_model.evaluate(X_train, y_train)
print(f'Test Loss: {test_loss:.4f}')
print(f'Test Accuracy: {test_accuracy:.4f}')

# Evaluate the model on the test set
test_loss, test_accuracy = dnn_model.evaluate(X_test, y_test)
print(f'Test Loss: {test_loss:.4f}')
print(f'Test Accuracy: {test_accuracy:.4f}')

# Evaluate the model on the test set
y_pred = (dnn_model.predict(X_test) > 0.5).astype(int)
roc_auc = roc_auc_score(y_test, y_pred)
print(f'AUC Score: {roc_auc:.4f}')

!pip install keras-tuner

"""# Optimizing the Model using Kerastuner"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense
from tensorflow import keras
import kerastuner as kt

# Define the hyperparameters and the search space for the Keras Tuner

def model_builder(hp):
    inputs = Input(shape=(X_train.shape[1],))

    # Tune the number of units in the first Dense layer
    hp_units = hp.Int('units', min_value=32, max_value=128, step=32)
    x = Dense(units=hp_units, activation='relu')(inputs)

    # Tune the learning rate for the optimizer
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])

    outputs = Dense(1, activation='sigmoid')(x)

    model = keras.Model(inputs=inputs, outputs=outputs)

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = kt.Hyperband(model_builder,
                     objective='val_accuracy',
                     max_epochs=10,
                     factor=3,
                     directory='my_dir',
                     project_name='intro_to_kt')

# Search for the best hyperparameters using Keras Tuner
tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))

# Get the best hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

# Build the tuned model and Train the tuned model
tuned_model = tuner.hypermodel.build(best_hps)
history = tuned_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

# Evaluate the model on the train set

test_loss, test_accuracy = tuned_model.evaluate(X_train, y_train)
print(f'Test Loss: {test_loss:.4f}')
print(f'Trained Accuracy: {test_accuracy:.4f}')

# Evaluate the model on the test set
test_loss, test_accuracy = tuned_model.evaluate(X_test, y_test)
print(f'Test Loss: {test_loss:.4f}')
print(f'Test Accuracy: {test_accuracy:.4f}')

# Evaluate the model on the test set
y_pred = (tuned_model.predict(X_test) > 0.5).astype(int)
roc_auc = roc_auc_score(y_test, y_pred)
print(f'AUC Score: {roc_auc:.4f}')

import matplotlib.pyplot as plt

# Access the training history
accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
epochs = range(1, len(accuracy) + 1)

# Plot the training and validation accuracy
plt.figure(figsize=(12, 6))
plt.plot(epochs, accuracy, 'b', label='Training Accuracy')
plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()
plt.show()

"""## SAVING THE MODEL

---

"""

from tensorflow.keras.models import load_model

"""#Saving the scalar



"""

import pickle

# pickle.dump(dnn_model,open('/content/dnn_model_saved','wb'))
tuned_model.save("model.h5")

## load the saved model to use for prediction
Model_load=pickle.load(open('/content/dnn_model_saved','rb'))

y_pred = (dnn_model.predict(X_test) > 0.5).astype(int)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'AUC Score: {roc_auc:.4f}')

import pickle
with open('scaler.pkl', 'wb') as file:
  pickle.dump(scaler, file)